## 轻量级爬虫

1. 爬虫简介
2. 简单爬虫架构
3. URL管理器
4. 网页下载器(urllib2)
5. 网页解析器(BeautifulSoup)
6. 完整实例
   - 爬取百度百科Python词条相关的1000个页面数据





## 爬虫简介

![](http://p7bj6aatj.bkt.clouddn.com/%E7%88%AC%E8%99%AB_%E7%AE%80%E4%BB%8B.png)

## 爬虫技术的价值

- 价值:互联网数据，为我所用

## 3-1 Python简单爬虫架构



![](http://p7bj6aatj.bkt.clouddn.com/%E7%AE%80%E5%8D%95%E7%88%AC%E8%99%AB%E6%9E%B6%E6%9E%84.png)



## 3-2 Python简单爬虫架构的动态运行流程

![](http://p7bj6aatj.bkt.clouddn.com/%E7%AE%80%E5%8D%95%E7%88%AC%E8%99%AB%E6%9E%B6%E6%9E%84_%E8%BF%90%E8%A1%8C%E6%B5%81%E7%A8%8B.png)

## 4-1 Python爬虫URL管理

![](http://p7bj6aatj.bkt.clouddn.com/URL_%E7%AE%A1%E7%90%86%E5%99%A8.png)

## 4-2 Python爬虫URL管理器的实现方式

![](http://p7bj6aatj.bkt.clouddn.com/URL_%E7%AE%A1%E7%90%86%E5%99%A8%E7%9A%84%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F.png)

## 5-1 Python爬虫网页下载器简介



![](http://p7bj6aatj.bkt.clouddn.com/%E7%BD%91%E9%A1%B5%E4%B8%8B%E8%BD%BD%E5%99%A8.png)





### **Python有哪几种网页下载器**

- ###### urllib2  Python官方的基础模块

- ###### requests  第三方包更强大



## 5-2 Python爬虫urllib2下载器网页的三种方法

环境:Python3.6

注意在python3中`import urllib.request`替代了`import urllib2`

###### 方法1:

~~~python
import urllib.request

#直接请求
response = urllib.request.urlopen('http://www.baidu.com')
#获取状态码，如果是200表示成功
print(response.getcode())
#读取内容
cont = response.read()
~~~

###### 方法2:

~~~python
#添加data、http header

import urllib.request

#创建Request对象
request = urllib.request.Request(url)

#添加数据
request.add_data('a','1')
#添加http的header
request.add_header('User-Agent','Mozilla/5.0')

#发送请求获取结果
response = urllib.request.urlopen(request)
~~~

###### 方法3:

![](http://p7bj6aatj.bkt.clouddn.com/%E7%BD%91%E9%A1%B5%E4%B8%8B%E8%BD%BD%E5%99%A8_%E6%96%B9%E6%B3%953.png)

~~~python
import urllib.request
import http.cookiejar

#创建cookie容器
cj = http.cookiejar.CookieJar()

#创建1个opener
opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cj))

#给urllib2安装opener
urllib.request.install_opener(opener)

#使用带有cookie的urllib访问页面
response = urllib.request.urlopen(url)
~~~

## 5-3 Python爬虫urllib2实例代码演示

~~~python
import urllib.request
import http.cookiejar

url = "http://www.baidu.com"


print('第一种方法')
response1 = urllib.request.urlopen(url)
print(response1.getcode())
print(len(response1.read()))

print('第二种方式')
request = urllib.request.Request(url)
request.add_header("User-Agent","Mozilla/5.0")
response2 = urllib.request.urlopen(request)
print(response2.getcode())
print(len(response2.read()))

print('第三种方式')
cj = http.cookiejar.CookieJar()
opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cj))
urllib.request.install_opener(opener)
response3 = urllib.request.urlopen(url)
print(response3.getcode())
print(cj)
print(len(response3.read()))
~~~

**终端显示**

~~~
第一种方法
200
116907
第二种方式
200
117121
第三种方式
200
<CookieJar[<Cookie BAIDUID=3C94F66E5673727199E31870728D06C3:FG=1 for .baidu.com/>, <Cookie BIDUPSID=3C94F66E5673727199E31870728D06C3 for .baidu.com/>, <Cookie H_PS_PSSID=1457_26430_21122_26350_20929 for .baidu.com/>, <Cookie PSTM=1531993556 for .baidu.com/>, <Cookie BDSVRTM=0 for www.baidu.com/>, <Cookie BD_HOME=0 for www.baidu.com/>]>
117125
~~~

## 6-1 网页解析器

![](http://p7bj6aatj.bkt.clouddn.com/%E7%BD%91%E9%A1%B5%E8%A7%A3%E6%9E%90%E5%99%A8.png)





![](http://p7bj6aatj.bkt.clouddn.com/python%E7%BD%91%E9%A1%B5%E8%A7%A3%E6%9E%90%E5%99%A8.png)

###### **html.parser、Beautiful Soup、lxml都是结构化解析**

![](http://p7bj6aatj.bkt.clouddn.com/%E7%BB%93%E6%9E%84%E5%8C%96%E8%A7%A3%E6%9E%90.png)

## 6-2 BeautifulSoup模块介绍和安装

- Beautiful Soup

  -Python第三方库，用于HTML或XML中提取数据

  -官网: http://www.crummy.com/software/BeautifulSoup/

- 安装并测试beautifulsoup4

  -安装:`pip install beautifulsoup4`

  -测试:`import bs4`

## 6-3 BeautifulSoup的语法

![](http://p7bj6aatj.bkt.clouddn.com/BeautifulSoup%E8%AF%AD%E6%B3%95.png)



![](http://p7bj6aatj.bkt.clouddn.com/beautifulsoup%E8%AF%AD%E6%B3%9501.png)



### 创建BeautifulSoup对象

~~~
from bs4 import BeautifulSoup


#根据html页面字符串创建BeautifulSoup对象
soup = BeautifulSoup(
					html_doc,				# HTML文档字符串
					'html.parser'			# HTML解析器
					from_encoding='utf-8'	# HTML文档的编码
					)
~~~

### 搜索节点(find_all,find)

~~~
#方法:find_all(name,attrs,string)

#查找所有标签为a的节点
soup.find_all('a')

#查找所有标签为a，链接符合/view/123.htm形式的节点
soup.find_all('a',href='/view/123.htm')
soup.find_all('a',href=re.compile(r'/view/\d+\.htm'))

# 查找所有标签为div，class为abc，文字为Python的节点
soup.find_all('div',class_='abc',string='Python')	#class_ 是为了避免于python关键字class冲突
~~~

### 访问节点信息

~~~
# 得到节点: <a href="1.html">Python</a>

# 获取查找到的节点的标签名称
node.name

# 获取查找到的a节点的href属性
node['href']

# 获取查找到的a节点的链接文字
node.get_text()
~~~

## 6-4 BeautifulSoup实例测试

~~~python
import re

import bs4
# print(bs4)
# <module 'bs4' from 'E:\\locate_app\\develop\\Python3\\lib\\site-packages\\bs4\\__init__.py'>

from bs4 import BeautifulSoup

html_doc = """
<html><head><title>The Dormouse's story</title></head>
<body>
<p class="title"><b>The Dormouse's story</b></p>

<p class="story">Once upon a time there were three little sisters; and their names were
<a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,
<a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
<a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
and they lived at the bottom of a well.</p>

<p class="story">...</p>
"""

soup = BeautifulSoup(html_doc, 'html.parser')

print('获取到所有的链接')
links = soup.find_all('a')
for link in links:
    print(link.name, link['href'], link.get_text())

print('获取Lacie的链接')
link_node = soup.find('a', href="http://example.com/lacie")
print(link_node.name, link_node['href'], link_node.get_text())

print('正则匹配')
link_node = soup.find('a', href=re.compile(r"ill"))  # 模糊匹配
print(link_node.name, link_node['href'], link_node.get_text())

print('获取p段落文字')
p_node = soup.find('p', class_='title')
print(p_node.name, p_node.get_text())

~~~



**运行结果**

~~~
获取到所有的链接
a http://example.com/elsie Elsie
a http://example.com/lacie Lacie
a http://example.com/tillie Tillie
获取Lacie的链接
a http://example.com/lacie Lacie
正则匹配
a http://example.com/tillie Tillie
获取p段落文字
p The Dormouse's story

Process finished with exit code 0
~~~

## 7-1  Python爬虫实例-分析实例

![](http://p7bj6aatj.bkt.clouddn.com/%E5%AE%9E%E4%BE%8B%E7%88%AC%E8%99%AB.png)



~~~


入口页: https://baike.baidu.com/item/Python/407313
url格式:
	- 词条页面url:  /item/Guido%20van%20Rossum
				   /view/10812319.htm
数据格式:
	- 标题
	
	<dd class="lemmaWgt-lemmaTitle-title">
    <h1>Python</h1>
    <h2>（计算机程序设计语言）</h2>
    ......
    </dd>
    - 简介
    <div class="lemma-summary" label-module="lemmaSummary">***</div>
    页面编码:  UTF-8
~~~



## 7-2 调度程序





## 7-3 URL管理器



## 7-4 HTML下载器html_downloader



## 7-5 HTML解析器html_parser



## 7-6 HTML输出器



## 7-7 开始运行爬虫和爬取结果展示



